{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Uncovering World Events using Twitter Hashtags\n",
    "\n",
    "## ... and learning about Spark `DataFrames` along the way\n",
    "\n",
    "In this notebook, we will use temporal information about Twitter hashtags to discover trending topics and potentially uncover world events as they occurred. \n",
    "\n",
    "__Hand-in:__\n",
    "\n",
    "- __Due: 28.04.2020 23:59:59 CET__\n",
    "- `git push` your final verion to your group's Renku repository before the due\n",
    "- check if `Dockerfile`, `environment.yml` and `requirements.txt` are properly written\n",
    "- add necessary comments and discussion to make your codes readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "The idea here is that when an event is happening and people are having a conversation about it on Twitter, a set of uniform hashtags that represent the event spontaneously evolves. Twitter users then use those hashtags to communicate with one another. Some hashtags, like `#RT` for \"retweet\" or just `#retweet` are used frequently and don't tell us much about what is going on. But a sudden appearance of a hashtag like `#oscars` probably indicates that the oscars are underway. For a particularly cool example of this type of analysis, check out [this blog post about earthquake detection using Twitter data](https://blog.twitter.com/official/en_us/a/2015/usgs-twitter-data-earthquake-detection.html) (although they search the text and not necessarily hashtags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>522</td><td>application_1583239045420_4300</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1583239045420_4300/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e04_1583239045420_4300_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f72ed7c51d0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorMemory': '4G', 'executorCores': 4, 'numExecutors': 10, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>520</td><td>application_1583239045420_4298</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1583239045420_4298/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e04_1583239045420_4298_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>521</td><td>application_1583239045420_4299</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1583239045420_4299/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e04_1583239045420_4299_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>522</td><td>application_1583239045420_4300</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1583239045420_4300/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e04_1583239045420_4300_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>send_to_spark</td>\n",
       "    <td>%%send_to_spark -o variable -t str -n var</td>\n",
       "    <td>Sends a variable from local output to spark cluster.\n",
       "    <br/>\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-i VAR_NAME: Local Pandas DataFrame(or String) of name VAR_NAME will be available in the %%spark context as a \n",
       "          Spark dataframe(or String) with the same name.</li>\n",
       "        <li>-t TYPE: Specifies the type of variable passed as -i. Available options are:\n",
       "         `str` for string and `df` for Pandas DataFrame. Optional, defaults to `str`.</li>\n",
       "        <li>-n NAME: Custom name of variable passed as -i. Optional, defaults to -i variable name.</li>\n",
       "        <li>-m MAXROWS: Maximum amount of Pandas rows that will be sent to Spark. Defaults to 2500.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Set up the hashtag `DataFrame` (10 points / 60)\n",
    "\n",
    "We have prepared the hashtag data spanning the time from May to July, 2016. This is a significant time in modern European history, e.g. see [Brexit](https://en.wikipedia.org/wiki/Brexit). Lets see if we can see any interesting trends about these events in the Twitter data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.a: TODO (1 point/10)\n",
    "\n",
    "Load the **parquet** data from `/data/twitter/parquet/hashtags` into a Spark dataframe using the appropriate `SparkSession` method. \n",
    "\n",
    "Look at the first few rows of the dataset - note the timestamp and its units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "df = spark.read.parquet(\"/data/twitter/parquet/hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_s: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- hashtag: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----+-----------------+\n",
      "|timestamp_s|                id|lang|          hashtag|\n",
      "+-----------+------------------+----+-----------------+\n",
      "| 1464768480|737918607980670976|  tr|          lovebts|\n",
      "| 1464768480|737918607980670976|  tr|            방탄소년단|\n",
      "| 1464768480|737918607963873281|  ja|          tokyofm|\n",
      "| 1464768480|737918607976452096|  en|          SotoBot|\n",
      "| 1464768480|737918607976497152|  en|           Fenton|\n",
      "| 1464768480|737918607976497152|  en|     vintageglass|\n",
      "| 1464768480|737918607976497152|  en|        glassbell|\n",
      "| 1464768480|737918607959719941|  en|          vintage|\n",
      "| 1464768480|737918607959719941|  en|            1970s|\n",
      "| 1464768480|737918607959719941|  en|              mod|\n",
      "| 1464768481|737918612158189568|  ar|        ابن_الغيم|\n",
      "| 1464768481|737918612158156802|  en|creativitybooster|\n",
      "| 1464768481|737918612158156802|  en|    growthhacking|\n",
      "| 1464768481|737918612158156802|  en| entrepreneurship|\n",
      "| 1464768481|737918612158222336|  en|    jewelryonetsy|\n",
      "| 1464768481|737918612158222336|  en|          Jetteam|\n",
      "| 1464768481|737918612162392064|  en|       TeenChoice|\n",
      "| 1464768481|737918612162392064|  en| ChoiceMaleHottie|\n",
      "| 1464768481|737918612149829632|  en|       NowPlaying|\n",
      "| 1464768481|737918612149829632|  en|             Maui|\n",
      "+-----------+------------------+----+-----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined functions\n",
    "\n",
    "A neat trick of spark dataframes is that you can essentially use something very much like an RDD `map` method but without switching to the RDD. If you are familiar with database languages, this works very much like e.g. a user-defined function in SQL. \n",
    "\n",
    "So, for example, if we wanted to make a user-defined python function that returns the hashtags in lowercase, we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@functions.udf\n",
    "def lowercase(text):\n",
    "    \"\"\"Convert text to lowercase\"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @functions.udf is a \"decorator\" -- this is really handy python syntactic sugar and in this case is equivalent to:\n",
    "\n",
    "```python\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "    \n",
    "lowercase = functions.udf(lowercase)\n",
    "```\n",
    "\n",
    "It basically takes our function and adds to its functionality. In this case, it registers our function as a pyspark dataframe user-defined function (UDF).\n",
    "\n",
    "Using these UDFs is very straightforward and analogous to other Spark dataframe operations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|lowercase(hashtag)|\n",
      "+------------------+\n",
      "|           lovebts|\n",
      "|             방탄소년단|\n",
      "|           tokyofm|\n",
      "|           sotobot|\n",
      "|            fenton|\n",
      "|      vintageglass|\n",
      "|         glassbell|\n",
      "|           vintage|\n",
      "|             1970s|\n",
      "|               mod|\n",
      "|         ابن_الغيم|\n",
      "| creativitybooster|\n",
      "|     growthhacking|\n",
      "|  entrepreneurship|\n",
      "|     jewelryonetsy|\n",
      "|           jetteam|\n",
      "|        teenchoice|\n",
      "|  choicemalehottie|\n",
      "|        nowplaying|\n",
      "|              maui|\n",
      "+------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df.select(lowercase(df.hashtag)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a framework like Spark is all about understanding the ins and outs of how it functions and knowing what it offers. One of the cool things about the dataframe API is that many functions are already defined for you (turning strings into lowercase being one of them). \n",
    "\n",
    "### I.b: TODO (2 points / 10)\n",
    "\n",
    "Find the [Spark python API documentation](https://spark.apache.org/docs/latest/api/python/index.html). Look for the `sql` section and find the listing of `sql.functions`. Repeat the above (turning hashtags into lowercase) but use the built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   lower(hashtag)|\n",
      "+-----------------+\n",
      "|          lovebts|\n",
      "|            방탄소년단|\n",
      "|          tokyofm|\n",
      "|          sotobot|\n",
      "|           fenton|\n",
      "|     vintageglass|\n",
      "|        glassbell|\n",
      "|          vintage|\n",
      "|            1970s|\n",
      "|              mod|\n",
      "|        ابن_الغيم|\n",
      "|creativitybooster|\n",
      "|    growthhacking|\n",
      "| entrepreneurship|\n",
      "|    jewelryonetsy|\n",
      "|          jetteam|\n",
      "|       teenchoice|\n",
      "| choicemalehottie|\n",
      "|       nowplaying|\n",
      "|             maui|\n",
      "+-----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df.select(functions.lower(df.hashtag)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with a combination of these built-in functions and user-defined functions for the remainder of this homework. \n",
    "\n",
    "### I.c: TODO (3 points/10)\n",
    "\n",
    "Create `english_df` consisting of **lowercase** hashtags from only english-language tweets. In addition, convert the timestamp to a more readable format like this and name the column `date`:\n",
    "\n",
    "```\n",
    "2016-05-01 08:30:00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `english_df` should look something like this:\n",
    "\n",
    "```\n",
    "+-----------+----+------------+-------------------+\n",
    "|timestamp_s|lang|     hashtag|               date|\n",
    "+-----------+----+------------+-------------------+\n",
    "| 1464768480|  en|     sotobot|2016-06-01 10:08:00|\n",
    "| 1464768480|  en|      fenton|2016-06-01 10:08:00|\n",
    "| 1464768480|  en|vintageglass|2016-06-01 10:08:00|\n",
    "| 1464768480|  en|   glassbell|2016-06-01 10:08:00|\n",
    "| 1464768480|  en|     vintage|2016-06-01 10:08:00|\n",
    "+-----------+----+------------+-------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------------------+-------------------+\n",
      "|timestamp_s|lang|           hashtag|               date|\n",
      "+-----------+----+------------------+-------------------+\n",
      "| 1464768480|  en|           sotobot|2016-06-01 10:08:00|\n",
      "| 1464768480|  en|            fenton|2016-06-01 10:08:00|\n",
      "| 1464768480|  en|      vintageglass|2016-06-01 10:08:00|\n",
      "| 1464768480|  en|         glassbell|2016-06-01 10:08:00|\n",
      "| 1464768480|  en|           vintage|2016-06-01 10:08:00|\n",
      "| 1464768480|  en|             1970s|2016-06-01 10:08:00|\n",
      "| 1464768480|  en|               mod|2016-06-01 10:08:00|\n",
      "| 1464768481|  en| creativitybooster|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|     growthhacking|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|  entrepreneurship|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|     jewelryonetsy|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|           jetteam|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|        teenchoice|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|  choicemalehottie|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|        nowplaying|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|              maui|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|    24hnurburgring|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|           porsche|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|caymangt4clubsport|2016-06-01 10:08:01|\n",
      "| 1464768481|  en|   artificialgrass|2016-06-01 10:08:01|\n",
      "+-----------+----+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "english_df = df.select(df.timestamp_s, df.lang, functions.lower(df.hashtag).alias('hashtag'), \n",
    "                       functions.to_timestamp(df.timestamp_s).alias('date')).where(df.lang == 'en')\n",
    "english_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.d: TODO (4 points/10)\n",
    "\n",
    "We \"claim\" the data contains all twitters from May to July 2017, however, that's not true. Check which (date, hour) are in `english_df` and which are missing, e.g. 2016-05-01, 10h is in the dataframe while 2016-04-01, 10h not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|  (date, hour)|\n",
      "+--------------+\n",
      "|2016-05-01, 10|\n",
      "|2016-05-01, 11|\n",
      "|2016-05-01, 12|\n",
      "|2016-05-01, 13|\n",
      "|2016-05-01, 14|\n",
      "|2016-05-01, 15|\n",
      "|2016-05-01, 16|\n",
      "|2016-05-01, 17|\n",
      "|2016-05-01, 18|\n",
      "|2016-05-01, 19|\n",
      "|2016-05-01, 20|\n",
      "|2016-05-01, 21|\n",
      "|2016-05-01, 22|\n",
      "|2016-05-01, 23|\n",
      "| 2016-05-01, 8|\n",
      "| 2016-05-01, 9|\n",
      "| 2016-05-02, 0|\n",
      "|2016-05-02, 15|\n",
      "|2016-05-02, 16|\n",
      "|2016-05-02, 17|\n",
      "|2016-05-02, 18|\n",
      "|2016-05-02, 19|\n",
      "|2016-05-02, 20|\n",
      "|2016-05-02, 21|\n",
      "|2016-05-02, 22|\n",
      "|2016-05-02, 23|\n",
      "| 2016-05-02, 5|\n",
      "| 2016-05-02, 6|\n",
      "| 2016-05-02, 7|\n",
      "| 2016-05-03, 0|\n",
      "| 2016-05-03, 1|\n",
      "|2016-05-03, 10|\n",
      "|2016-05-03, 11|\n",
      "|2016-05-03, 12|\n",
      "|2016-05-03, 13|\n",
      "|2016-05-03, 14|\n",
      "|2016-05-03, 15|\n",
      "|2016-05-03, 16|\n",
      "|2016-05-03, 17|\n",
      "|2016-05-03, 18|\n",
      "|2016-05-03, 19|\n",
      "| 2016-05-03, 2|\n",
      "|2016-05-03, 20|\n",
      "|2016-05-03, 21|\n",
      "|2016-05-03, 22|\n",
      "|2016-05-03, 23|\n",
      "| 2016-05-03, 3|\n",
      "| 2016-05-03, 4|\n",
      "| 2016-05-03, 5|\n",
      "| 2016-05-03, 6|\n",
      "| 2016-05-03, 7|\n",
      "| 2016-05-03, 8|\n",
      "| 2016-05-03, 9|\n",
      "| 2016-05-04, 0|\n",
      "| 2016-05-04, 1|\n",
      "|2016-05-04, 10|\n",
      "|2016-05-04, 11|\n",
      "|2016-05-04, 12|\n",
      "|2016-05-04, 13|\n",
      "|2016-05-04, 14|\n",
      "|2016-05-04, 15|\n",
      "|2016-05-04, 16|\n",
      "|2016-05-04, 17|\n",
      "|2016-05-04, 18|\n",
      "|2016-05-04, 19|\n",
      "| 2016-05-04, 2|\n",
      "|2016-05-04, 20|\n",
      "|2016-05-04, 21|\n",
      "|2016-05-04, 22|\n",
      "|2016-05-04, 23|\n",
      "| 2016-05-04, 3|\n",
      "| 2016-05-04, 4|\n",
      "| 2016-05-04, 5|\n",
      "| 2016-05-04, 6|\n",
      "| 2016-05-04, 7|\n",
      "| 2016-05-04, 8|\n",
      "| 2016-05-04, 9|\n",
      "| 2016-05-05, 0|\n",
      "| 2016-05-05, 1|\n",
      "|2016-05-05, 10|\n",
      "|2016-05-05, 11|\n",
      "|2016-05-05, 12|\n",
      "|2016-05-05, 13|\n",
      "|2016-05-05, 14|\n",
      "|2016-05-05, 15|\n",
      "|2016-05-05, 16|\n",
      "|2016-05-05, 17|\n",
      "|2016-05-05, 18|\n",
      "|2016-05-05, 19|\n",
      "| 2016-05-05, 2|\n",
      "|2016-05-05, 20|\n",
      "|2016-05-05, 21|\n",
      "|2016-05-05, 22|\n",
      "|2016-05-05, 23|\n",
      "| 2016-05-05, 3|\n",
      "| 2016-05-05, 4|\n",
      "| 2016-05-05, 5|\n",
      "| 2016-05-05, 6|\n",
      "| 2016-05-05, 7|\n",
      "| 2016-05-05, 8|\n",
      "+--------------+\n",
      "only showing top 100 rows"
     ]
    }
   ],
   "source": [
    "# lit: takes the argument literally. E.g. for concatenating strings\n",
    "# Ordering of hours is as follows: 0, 1, 10, 11, ..., 19, 2, 20, 21, 22, 23, 3, 4, 5, ..., 9 (which makes perfect sense)\n",
    "english_df.select(functions.concat(functions.to_date(english_df.date),\n",
    "                                   functions.lit(', '), \n",
    "                                   functions.hour(english_df.date)).alias('(date, hour)')).distinct().orderBy('(date, hour)').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Twitter hashtag trends (50 points / 60)\n",
    "\n",
    "In this section we will try to do a slightly more complicated analysis of the tweets. Our goal is to get an idea of tweet frequency as a function of time for certain hashtags. \n",
    "\n",
    "Lets build this up in steps. First, lets see how we can start to organize the tweets by their timestamps. \n",
    "\n",
    "As a first easy example, lets say we just want to count the number of tweets per minute over the entire span of our data. For this, we first need a \"global\" minute value, e.g. \"minute of the year\" or something similar. \n",
    "\n",
    "Spark provides us with some handy built-in dataframe functions that are made for transforming date and time fields. \n",
    "\n",
    "Have a look [here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) to see the whole list of custom dataframe functions - you will need to use them to complete the next set of TODO items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the functions can be combined. Consider the following dataframe and its transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create a sample dataframe with one column \"degrees\" going from 0 to 180\n",
    "test_df = spark.createDataFrame(spark.sparkContext.range(180).map(lambda x: Row(degrees=x)), ['degrees'])\n",
    "\n",
    "# define a function \"sin_rad\" that first converts degrees to radians and then takes the sine using built-in functions\n",
    "sin_rad = functions.sin(functions.radians(test_df.degrees))\n",
    "\n",
    "# show the result\n",
    "test_df.select(sin_rad).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames `groupBy`\n",
    "\n",
    "We used `groupBy` already in the previous notebook, but here we will take more advantage of its features. \n",
    "\n",
    "One important thing to note is that unlike other RDD or DataFrame transformations, the `groupBy` does not return another DataFrame, but a `GroupedData` object instead, with its own methods. These methods allow you to do various transformations and aggregations on the data of the grouped rows. \n",
    "\n",
    "Conceptually the procedure is a lot like this:\n",
    "\n",
    "![groupby](https://i.stack.imgur.com/sgCn1.jpg)\n",
    "\n",
    "The column that is used for the `groupBy` is the `key` - once we have the values of a particular key all together, we can use various aggregation functions on them to generate a transformed dataset. In this example, the aggregation function is a simple `sum`. In the simple procedure below, the `key` will be the hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.a: TODO (1 point / 50)\n",
    "\n",
    "Calculate the top five most common hashtags in the whole english-language dataset.\n",
    "\n",
    "This should be your result:\n",
    "\n",
    "```\n",
    "+-------------+------+\n",
    "|      hashtag| count|\n",
    "+-------------+------+\n",
    "|   mtvhottest|800527|\n",
    "|veranomtv2016|539028|\n",
    "|   teenchoice|345208|\n",
    "|   nowplaying|178561|\n",
    "|  gameinsight|165237|\n",
    "+-------------+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily hashtag trends\n",
    "\n",
    "Now we will start to complicate the analysis a bit. Remember, our goal is to uncover trending topics on a timescale of a few days. A much needed column then is simply `day`. To convert the date string into day-of-year, you can use the built-in [dayofyear](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofyear) function. \n",
    "\n",
    "In the subsequent sections we will then not only see which hashtags are globally most popular, but which ones experience the biggest changes in popularity - those are the \"trending\" topics. If there is suddenly a substantial increase of a hashtag over a matter of a day or two, it may signify an event taking place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.b: TODO (2 points / 50)\n",
    "\n",
    "Create a dataframe called `daily_hashtag` that includes the columns `month`, `week`, `day` and `hashtag`. Use the `english_df` you made above to start, and make sure you find the appropriate spark dataframe functions to make your life easier. Show the result.\n",
    "\n",
    "Try to match this view:\n",
    "\n",
    "```\n",
    "+-----+----+---+------------+\n",
    "|month|week|day|     hashtag|\n",
    "+-----+----+---+------------+\n",
    "|    6|  22|153|     sotobot|\n",
    "|    6|  22|153|      fenton|\n",
    "|    6|  22|153|vintageglass|\n",
    "|    6|  22|153|   glassbell|\n",
    "|    6|  22|153|     vintage|\n",
    "+-----+----+---+------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.c: TODO (2 points / 50)\n",
    "\n",
    "Now we want to calculate the number of times a hashtag is used per day. Sort in descending order of daily counts and show the result. Call the resulting dataframe `day_counts`.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "```\n",
    "+---+----------+----+-----+\n",
    "|day|   hashtag|week|count|\n",
    "+---+----------+----+-----+\n",
    "|204|mtvhottest|  29|66372|\n",
    "|205|mtvhottest|  29|63495|\n",
    "|207|mtvhottest|  30|60768|\n",
    "|208|mtvhottest|  30|59065|\n",
    "|199|mtvhottest|  28|57956|\n",
    "+---+----------+----+-----+\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<p>Make sure you use `cache()` when you create `day_counts` because we will need it in the steps that follow!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.d: TODO (2 points / 50)\n",
    "\n",
    "To get an idea of which hashtags stay popular for several days, calculate the average number of daily occurences for each week. \n",
    "\n",
    "__Hint:__ use the `week` column we created above. Sort in descending order and show the top 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `Window` functions \n",
    "\n",
    "Window functions are another awesome feature of dataframes. They allow users to accomplish complex tasks using very concise and simple code. \n",
    "\n",
    "Above we computed just the hashtag that had the most occurrences on *any* day. Now lets say we want to know the top tweets for *each* day.  \n",
    "\n",
    "This is a non-trivial thing to compute and requires \"windowing\" our data. I recommend reading this [window functions article](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html) to get acquainted with the idea. You can think of a window function as a fine-grained and more flexible `groupBy`. \n",
    "\n",
    "There are two things we need to define to use window functions:\n",
    "\n",
    "1. the \"window\" to use, based on which columns (partitioning) and how the rows should be ordered \n",
    "2. the computation to carry out for each windowed group, e.g. a max, an average etc.\n",
    "\n",
    "Lets see how this works by example. We will define a window function, `daily_window` that will partition data based on the `day` column. Within each window, the rows will be ordered by the daily hashtag count that we computed above. Finally, we will use the rank function **over** this window to give us the ranking of top tweets. \n",
    "\n",
    "In the end, this is a fairly complicated operation achieved in just a few lines of code! (can you think of how to do this with an RDD??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify the window function and the ordering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_window = Window.partitionBy('day').orderBy(functions.desc('count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above window function says that we should window the data on the `day` column and order it by count. \n",
    "\n",
    "Now we need to define what we want to compute on the windowed data. We will start by just calculating the daily ranking of hashtags, so we can use the helpful built-in `functions.rank()` and sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_rank = functions.rank() \\\n",
    "                      .over(daily_window) \\\n",
    "                      .alias('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.e: TODO (3 points / 50)\n",
    "Now compute the top five hashtags for each day in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.f: TODO - Compute the top five trending tweets per week (20 points / 50)\n",
    "\n",
    "Use window functions (or other techniques!) to produce lists of top few trending tweets for each week. What's a \"trending\" tweet? Something that seems to be suddenly growing very rapidly in popularity. You should identify \"brexit\" in week 25 and other events like the death of The Artist Formerly Known as Prince, the Met gala, Euro 2016, the terrorist attacks in Nice, France etc. Make it as simple or as complicated as you want! The final listing should be clear and concise and the flow of your analysis should be easy to follow. If you make an implementation that is not immediately obvious, make sure you provide comments either in markdown cells or in comments in the code itself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the timeseries of hashtag counts and using KMeans clustering\n",
    "\n",
    "### II.g: TODO (8 points / 50)\n",
    "\n",
    "The lists we obtained above are nice, but lets actually visualize some data. \n",
    "\n",
    "1. create a matrix that consists of hashtags as rows and daily counts as columns (hint: use `groupBy` and methods of `GroupedData`). \n",
    "2. use the `VectorAssembler` from the Spark ML library to create the feature vector which will consist of daily counts. \n",
    "\n",
    "If you extract any of these vectors you will obtain an array that represents the time series of daily counts - plot this time series for a few interesting hashtags you identified above. \n",
    "\n",
    "__Hint__: `isin` method of DataFrame columns might be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.h: TODO - Use KMeans to cluster hashtags based on the daily count timeseries (12 points / 50)\n",
    "\n",
    "Use the DataFrame you created above to cluster the hashtag timeseries. Train the model and calculate the cluster membership for all hashtags. \n",
    "\n",
    "Show the cluster that includes \"brexit\" - does it make sense?\n",
    "\n",
    "Again, be creative and see if you can get the clustering to give you meaningful hashtag groupings. Make sure you document your process and code and make your final notebook easy to understand even if the result is not optimal or complete. \n",
    "\n",
    "__Hint:__ Additional data cleaning or filtering might be necessary to get useful results from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
